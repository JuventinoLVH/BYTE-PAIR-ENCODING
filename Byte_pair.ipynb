{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BYTE-PAIR-ENCODING del Quijote\n",
    "--- \n",
    "Este es una forma simple de compresión de datos en la que el par más común de bytes consecutivos de datos se reemplaza con un byte que no ocurre dentro de esos datos. Aquí, el objetivo no es la compresión de datos, sino la codificación de texto en un idioma dado como una secuencia de 'tokens', utilizando un vocabulario fijo de diferentes tokens. La mayoría de las palabras se codificarán como un solo token, mientras que las palabras raras se codificarán como una secuencia de unos pocos tokens, donde estos tokens representan partes de palabras significativas.\n",
    "\n",
    "### Equipo cangrejo      \n",
    "* Montaño Preciado Alondra Karolina\n",
    "* Velasquez Hidalgo Luis Juventino\n",
    "* Navarro Lopez Malcom Hiram\n",
    "* Faz Leal Juan Carlos\n",
    "\n",
    "\n",
    "### Fuentes\n",
    "- Medium con la informacion: https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0\n",
    "\n",
    "- Codigo en el cual nos estamos inspirando: https://leimao.github.io/blog/Byte-Pair-Encoding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, clean_type='all'):\n",
    "    if clean_type == 'commas':\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    elif clean_type == 'numbers':\n",
    "        text = re.sub(r'\\d', '', text)\n",
    "    else:\n",
    "        text = re.sub(r'[^\\w\\s]|\\d', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " START OF THIS PROJECT GUTENBERG EBOOK DON QUIJOTE \n",
      "Posting Date April   EBook \n",
      "YO EL REY\n",
      "Por Dios hermano vuestras aciones\n",
      "En lo que toca el poner anotaciones al fin del libro\n",
      "Para qué conmigo flo\n"
     ]
    }
   ],
   "source": [
    "print(clean_text(\"*** START OF THIS PROJECT GUTENBERG EBOOK DON QUIJOTE ***\"))\n",
    "print(clean_text(\"Posting Date: April 27, 2010 [EBook #2000]\"))\n",
    "print(clean_text(\"YO, EL REY.\"))\n",
    "print(clean_text(\"-Por Dios, hermano, vuestras aciones.\"))\n",
    "print(clean_text(\"»En lo que toca el poner anotaciones al fin del libro,\"))\n",
    "print(clean_text(\"''¿Para qué conmigo flo-?''\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_vocab()\n",
    "___\n",
    "La función get_vocab lee un archivo de texto (especificado por filename) y devuelve un diccionario de frecuencias de palabras (vocabulario).\n",
    "Cada línea en el archivo de texto se divide en palabras y cada palabra se agrega al vocabulario con un contador de frecuencia inicial de 1.\n",
    "Si la palabra ya existe en el vocabulario, su contador se incrementa en 1.\n",
    "\n",
    "- filename: Es un argumento de entrada para la función, es el nombre (o la ruta) del archivo de texto que se va a leer para crear el vocabulario.\n",
    "- return: Es una instrucción que se utiliza para devolver un valor desde la funcion, en este caso, la función get_vocab devuelve el diccionario de frecuencias\n",
    "  de palabras (vocabulario) creado en el cuerpo de la función, que se almacena en la variable vocab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(filename):\n",
    "    vocab = collections.defaultdict(int)\n",
    "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
    "        for line in fhand:\n",
    "            line = clean_text(line)\n",
    "            words = line.strip().split()\n",
    "            for word in words:\n",
    "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_stats()\n",
    "___\n",
    "La función get_stats toma un vocabulario, el cual tiene la forma de un diccionario, y retorna otro diccionario con los pares de simbolos de nuestro vocabulario y la frecuencia con la que estos pares aparecen. Si el par ya existe en el diccionario, su contador se incrementa en 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge_vocab()\n",
    "___\n",
    "Funcion que junta los dos tokens que mas aparecen consecutivamente en el vocabulatio.\n",
    "El bigrama es la union de los dos tokens que mas aparecen. \n",
    "\n",
    "Si la palabra contiene el bigrama, \"w_out\" obtiene el valor de la palabra actual remplazando los dos tokens por el bigrama, de lo contrario \"w_out\" obtiene conserva el valor de la palabra de \n",
    "\n",
    "la instruccion:\n",
    "`v_out[w_out] = v_in[word]`\n",
    "nos asegura que la frecuencia de la *posible* nueva palabra se conserva.\n",
    "- pari: El par de tokens que mas se repiten  \n",
    "- v_in: El vocabulario actual de palabras.\n",
    "- return: El nuevo vocabulario, donde la ocurrencia del par de tokens 'pair' se remplazo por su union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_tokens_from_vocab()\n",
    "___\n",
    "Es una función que produce dos objetos a partir de un vocabulario. Uno de los objetos contendra un diccionario que asocia cada símbolo individual en el vocabulario a la suma de las frecuencias de todas las palabras que contienen ese token. El otro objeto también es un diccionario pero este asocia acada palabra del vocabulario una lista de tokens individuales.\n",
    "\n",
    "como un diccionario predeterminado y luego recorre cada entrada en vocabulario. Para cada entrada, la función divide la palabra en tokens individuales y agrega la frecuencia de la palabra a la frecuencia total de cada token. Además, la función agrega una entrada al diccionario que asocia la palabra completa con su lista de tokens. Finalmente, la función devuelve ambos diccionarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_from_vocab(vocab):\n",
    "    # se crea un diccionario de frecuencias y de tokenizacion de tokens vacio\n",
    "    tokens_frequencies = collections.defaultdict(int)\n",
    "    vocab_tokenization = {}\n",
    "    # se recorre el vocabulario de palabras y frecuencias de palabras del corpus\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens_frequencies[token] += freq\n",
    "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
    "        # se devuelve el diccionario de frecuencias de tokens y el diccionario de tokenizacion de tokens\n",
    "    return tokens_frequencies, vocab_tokenization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### measure_token_length()\n",
    "___\n",
    "\n",
    "La función measure_token_lengt mide la longitud de un token de texto. Si el token termina en </w>, se devuelve\n",
    "la longitud de la palabra sin el sufijo </w> más 1. \n",
    "Si el token no termina en </w>, se devuelve la longitud del token\n",
    "tal como está. Esto significa que la función devuelve la longitud de la palabra real o la longitud de la palabra más el sufijo </w>.\n",
    "\n",
    "-token: Es un argumento de entrada para la función measure_token_length. Es una cadena de texto que\n",
    "representa una palabra o una secuencia de caracteres. \n",
    "\n",
    "La función mide su longitud y devuelve el resultado.\n",
    "-return len(token): Es una instrucción que devuelve la longitud de la cadena de texto token. La función len en\n",
    "python devuelve la cantidad de caracteres en una cadena de texto.\n",
    "En este caso, si token no termina en </w>, se devuelve su longitud usando return len(token). De lo contrario,\n",
    "se devuelve len(token[:-4]) + 1, lo que representa la longitud de token sin los últimos 4\n",
    "caracteres (que son </w>) más 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_token_length(token):\n",
    "    if token[-4:] == '</w>':\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        return len(token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize_word()\n",
    "___\n",
    "Es una función de tokenización de palabras basada en el algoritmo de búsqueda de coincidencia de una lista de tokens conocidos. Toma como entrada una cadena de texto y una lista de tokens conocidos ordenados. Si la cadena de texto y la lista de tokens conocidos no son vacías, se itera sobre la lista de tokens conocidos en orden y se buscan coincidencias con la cadena de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Funcion para tokenizar una palabra con un vocabulario de tokens ordenados por frecuencia y longitud de token ###\n",
    "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
    "    \n",
    "    if string == '':\n",
    "        return []\n",
    "    \n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token]\n",
    "    string_tokens = []\n",
    "    # se recorre el vocabulario de tokens ordenados por frecuencia y longitud de token #\n",
    "    for i in range(len(sorted_tokens)):\n",
    "    \n",
    "        token = sorted_tokens[i]\n",
    "        \n",
    "        # se reemplaza el punto por [.] para que no sea interpretado como cualquier caracter #\n",
    "        token_reg = re.escape(token.replace('.', '[.]'))\n",
    "        \n",
    "        # se busca el token en la palabra y se obtienen las posiciones de inicio y fin de cada ocurrencia #\n",
    "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
    "        \n",
    "        #si no se encuentra el token en la palabra se continua con el siguiente token #\n",
    "        if len(matched_positions) == 0:\n",
    "            continue\n",
    "            # si se encuentra el token en la palabra se obtienen las posiciones de inicio y fin de cada ocurrencia #\n",
    "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
    "\n",
    "        # se recorre la palabra desde el inicio hasta la posicion de inicio de cada ocurrencia del token #\n",
    "        substring_start_position = 0\n",
    "        \n",
    "        for substring_end_position in substring_end_positions:\n",
    "            # se obtiene la subcadena desde la posicion de inicio de la palabra hasta la posicion de inicio de la ocurrencia del token #\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "            string_tokens += [token]\n",
    "            #se actualiza la posicion de inicio de la palabra para continuar desde la posicion de fin de la ocurrencia del token #\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        # se tokeniza la subcadena restante de la palabra #\n",
    "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "        break\n",
    "    # se devuelve la lista de tokens de la palabra #\n",
    "    return string_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementacion usando el Quijote\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui implementamos el codigo de Byte-pair-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Tokens Before BPE\n",
      "All tokens: dict_keys(['T', 'h', 'e', '</w>', 'P', 'r', 'o', 'j', 'c', 't', 'G', 'u', 'n', 'b', 'g', 'E', 'B', 'k', 'f', 'D', 'Q', 'i', 'y', 'M', 'l', 'd', 'C', 'v', 'a', 's', 'S', 'w', 'm', 'Y', 'p', 'L', 'A', 'R', 'O', 'F', 'H', 'I', 'J', 'U', 'N', 'K', 'x', 'q', 'á', 'ñ', 'é', 'í', 'V', 'ó', 'ú', 'z', 'É', 'Ó', 'X', 'Z', 'ü', 'Á', 'Í'])\n",
      "Number of tokens: 63\n",
      "==========\n",
      "Iter: 0\n",
      "Best pair: ('e', '</w>')\n",
      "All tokens: dict_keys(['T', 'h', 'e</w>', 'P', 'r', 'o', 'j', 'e', 'c', 't', '</w>', 'G', 'u', 'n', 'b', 'g', 'E', 'B', 'k', 'f', 'D', 'Q', 'i', 'y', 'M', 'l', 'd', 'C', 'v', 'a', 's', 'S', 'w', 'm', 'Y', 'p', 'L', 'A', 'R', 'O', 'F', 'H', 'I', 'J', 'U', 'N', 'K', 'x', 'q', 'á', 'ñ', 'é', 'í', 'V', 'ó', 'ú', 'z', 'É', 'Ó', 'X', 'Z', 'ü', 'Á', 'Í'])\n",
      "Number of tokens: 64\n",
      "==========\n",
      "Iter: 1\n",
      "Best pair: ('o', '</w>')\n",
      "All tokens: dict_keys(['T', 'h', 'e</w>', 'P', 'r', 'o', 'j', 'e', 'c', 't', '</w>', 'G', 'u', 'n', 'b', 'g', 'E', 'B', 'k', 'f', 'D', 'Q', 'i', 'y', 'M', 'l', 'd', 'C', 'v', 'a', 's', 'S', 'w', 'o</w>', 'm', 'Y', 'p', 'L', 'A', 'R', 'O', 'F', 'H', 'I', 'J', 'U', 'N', 'K', 'x', 'q', 'á', 'ñ', 'é', 'í', 'V', 'ó', 'ú', 'z', 'É', 'Ó', 'X', 'Z', 'ü', 'Á', 'Í'])\n",
      "Number of tokens: 65\n",
      "==========\n",
      "Iter: 2\n",
      "Best pair: ('s', '</w>')\n",
      "All tokens: dict_keys(['T', 'h', 'e</w>', 'P', 'r', 'o', 'j', 'e', 'c', 't', '</w>', 'G', 'u', 'n', 'b', 'g', 'E', 'B', 'k', 'f', 'D', 'Q', 'i', 'y', 'M', 'l', 'd', 'C', 'v', 'a', 's</w>', 'S', 's', 'w', 'o</w>', 'm', 'Y', 'p', 'L', 'A', 'R', 'O', 'F', 'H', 'I', 'J', 'U', 'N', 'K', 'x', 'q', 'á', 'ñ', 'é', 'í', 'V', 'ó', 'ú', 'z', 'É', 'Ó', 'X', 'Z', 'ü', 'Á', 'Í'])\n",
      "Number of tokens: 66\n",
      "==========\n",
      "Iter: 3\n",
      "Best pair: ('a', '</w>')\n",
      "All tokens: dict_keys(['T', 'h', 'e</w>', 'P', 'r', 'o', 'j', 'e', 'c', 't', '</w>', 'G', 'u', 'n', 'b', 'g', 'E', 'B', 'k', 'f', 'D', 'Q', 'i', 'y', 'M', 'l', 'd', 'C', 'v', 'a', 's</w>', 'S', 'a</w>', 's', 'w', 'o</w>', 'm', 'Y', 'p', 'L', 'A', 'R', 'O', 'F', 'H', 'I', 'J', 'U', 'N', 'K', 'x', 'q', 'á', 'ñ', 'é', 'í', 'V', 'ó', 'ú', 'z', 'É', 'Ó', 'X', 'Z', 'ü', 'Á', 'Í'])\n",
      "Number of tokens: 67\n",
      "==========\n",
      "Iter: 4\n",
      "Best pair: ('e', 'n')\n",
      "All tokens: dict_keys(['T', 'h', 'e</w>', 'P', 'r', 'o', 'j', 'e', 'c', 't', '</w>', 'G', 'u', 'en', 'b', 'g', 'E', 'B', 'k', 'f', 'D', 'n', 'Q', 'i', 'y', 'M', 'l', 'd', 'C', 'v', 'a', 's</w>', 'S', 'a</w>', 's', 'w', 'o</w>', 'm', 'Y', 'p', 'L', 'A', 'R', 'O', 'F', 'H', 'I', 'J', 'U', 'N', 'K', 'x', 'q', 'á', 'ñ', 'é', 'í', 'V', 'ó', 'ú', 'z', 'É', 'Ó', 'X', 'Z', 'ü', 'Á', 'Í'])\n",
      "Number of tokens: 68\n",
      "==========\n",
      "Iter: 5\n",
      "Best pair: ('q', 'u')\n",
      "All tokens: dict_keys(['T', 'h', 'e</w>', 'P', 'r', 'o', 'j', 'e', 'c', 't', '</w>', 'G', 'u', 'en', 'b', 'g', 'E', 'B', 'k', 'f', 'D', 'n', 'Q', 'i', 'y', 'M', 'l', 'd', 'C', 'v', 'a', 's</w>', 'S', 'a</w>', 's', 'w', 'o</w>', 'm', 'Y', 'p', 'L', 'A', 'R', 'O', 'F', 'H', 'I', 'J', 'U', 'N', 'K', 'x', 'qu', 'á', 'ñ', 'é', 'í', 'V', 'ó', 'ú', 'z', 'É', 'Ó', 'X', 'Z', 'ü', 'Á', 'Í'])\n",
      "Number of tokens: 68\n",
      "==========\n",
      "Iter: 6\n",
      "Best pair: ('o', 's</w>')\n",
      "All tokens: dict_keys(['T', 'h', 'e</w>', 'P', 'r', 'o', 'j', 'e', 'c', 't', '</w>', 'G', 'u', 'en', 'b', 'g', 'E', 'B', 'k', 'f', 'D', 'n', 'Q', 'i', 'y', 'M', 'l', 'd', 'C', 'v', 'a', 's</w>', 'S', 'a</w>', 's', 'w', 'o</w>', 'm', 'Y', 'p', 'L', 'A', 'R', 'O', 'F', 'H', 'I', 'J', 'U', 'N', 'K', 'x', 'qu', 'á', 'ñ', 'os</w>', 'é', 'í', 'V', 'ó', 'ú', 'z', 'É', 'Ó', 'X', 'Z', 'ü', 'Á', 'Í'])\n",
      "Number of tokens: 69\n",
      "==========\n",
      "Iter: 7\n",
      "Best pair: ('d', 'e</w>')\n",
      "All tokens: dict_keys(['T', 'h', 'e</w>', 'P', 'r', 'o', 'j', 'e', 'c', 't', '</w>', 'G', 'u', 'en', 'b', 'g', 'E', 'B', 'k', 'f', 'D', 'n', 'Q', 'i', 'y', 'M', 'l', 'de</w>', 'C', 'v', 'a', 's</w>', 'S', 'd', 'a</w>', 's', 'w', 'o</w>', 'm', 'Y', 'p', 'L', 'A', 'R', 'O', 'F', 'H', 'I', 'J', 'U', 'N', 'K', 'x', 'qu', 'á', 'ñ', 'os</w>', 'é', 'í', 'V', 'ó', 'ú', 'z', 'É', 'Ó', 'X', 'Z', 'ü', 'Á', 'Í'])\n",
      "Number of tokens: 70\n",
      "==========\n",
      "Iter: 8\n",
      "Best pair: ('l', '</w>')\n",
      "All tokens: dict_keys(['T', 'h', 'e</w>', 'P', 'r', 'o', 'j', 'e', 'c', 't', '</w>', 'G', 'u', 'en', 'b', 'g', 'E', 'B', 'k', 'f', 'D', 'n', 'Q', 'i', 'y', 'M', 'l</w>', 'de</w>', 'C', 'v', 'a', 's</w>', 'S', 'd', 'a</w>', 's', 'w', 'o</w>', 'l', 'm', 'Y', 'p', 'L', 'A', 'R', 'O', 'F', 'H', 'I', 'J', 'U', 'N', 'K', 'x', 'qu', 'á', 'ñ', 'os</w>', 'é', 'í', 'V', 'ó', 'ú', 'z', 'É', 'Ó', 'X', 'Z', 'ü', 'Á', 'Í'])\n",
      "Number of tokens: 71\n",
      "==========\n",
      "Iter: 9\n",
      "Best pair: ('e', 'r')\n",
      "All tokens: dict_keys(['T', 'h', 'e</w>', 'P', 'r', 'o', 'j', 'e', 'c', 't', '</w>', 'G', 'u', 'en', 'b', 'er', 'g', 'E', 'B', 'k', 'f', 'D', 'n', 'Q', 'i', 'y', 'M', 'l</w>', 'de</w>', 'C', 'v', 'a', 's</w>', 'S', 'd', 'a</w>', 's', 'w', 'o</w>', 'l', 'm', 'Y', 'p', 'L', 'A', 'R', 'O', 'F', 'H', 'I', 'J', 'U', 'N', 'K', 'x', 'qu', 'á', 'ñ', 'os</w>', 'é', 'í', 'V', 'ó', 'ú', 'z', 'É', 'Ó', 'X', 'Z', 'ü', 'Á', 'Í'])\n",
      "Number of tokens: 72\n",
      "==========\n",
      "['os</w>', 'de</w>', 'o</w>', 'a</w>', 'e</w>', 'en', 's</w>', 'qu', 'l</w>', 'er', 'a', '</w>', 'i', 'e', 'r', 'n', 't', 'l', 'o', 'c', 's', 'd', 'u', 'm', 'p', 'b', 'y', 'g', 'h', 'v', 'f', 'á', 'é', 'j', 'í', 'ó', 'E', 'z', 'A', 'D', 'C', 'P', 'T', 'M', 'ñ', 'S', 'L', 'O', 'Y', 'Q', 'R', 'G', 'B', 'J', 'N', 'ú', 'I', 'w', 'U', 'x', 'V', 'F', 'H', 'k', 'Z', 'K', 'É', 'Ó', 'X', 'ü', 'Á', 'Í']\n",
      "Tokenizing word: medio</w>...\n",
      "Tokenization of the known word:\n",
      "['m', 'e', 'd', 'i', 'o</w>']\n",
      "Tokenization treating the known word as unknown:\n",
      "['m', 'e', 'd', 'i', 'o</w>']\n",
      "Tokenizing word: Ilikeeatingapples!</w>...\n",
      "Tokenizating of the unknown word:\n",
      "['I', 'l', 'i', 'k', 'e', 'e', 'a', 't', 'i', 'n', 'g', 'a', 'p', 'p', 'l', 'e', 's', '</w>']\n"
     ]
    }
   ],
   "source": [
    "vocab = get_vocab('primer_capitulo.txt')\n",
    "\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "print('==========')\n",
    "\n",
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "    print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "    print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "    print('==========')\n",
    "\n",
    "# Let's check how tokenization will be for a known word\n",
    "word_given_known = 'medio</w>'\n",
    "word_given_unknown = 'Ilikeeatingapples!</w>'\n",
    "\n",
    "# Ordena el diccionario tokens_frequencies usando la combinacion de dos criterios:\n",
    "# 1. La llamada a la funcion measure_token_length\n",
    "# 2. La frecuencia de los tokens.\n",
    "# Los tokens con una longitud mayor en el resultado de measure_token_length serán\n",
    "# primero en la lista, y si hay varios tokens con la misma longitud, entonces aquellos\n",
    "# con una frecuencia más alta serán primero en la lista.\n",
    "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
    "# Es una lista de los tokens ordenados en función de la longitud de los tokens y sus frecuencias\n",
    "# donde los tokens con mayor longitud y mayor frecuencia se encuentran en la parte superior de la lista.\n",
    "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
    "\n",
    "print(sorted_tokens)\n",
    "\n",
    "word_given = word_given_known \n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_given))\n",
    "if word_given in vocab_tokenization:\n",
    "    print('Tokenization of the known word:')\n",
    "    print(vocab_tokenization[word_given])\n",
    "    print('Tokenization treating the known word as unknown:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "else:\n",
    "    print('Tokenizating of the unknown word:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "\n",
    "word_given = word_given_unknown \n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_given))\n",
    "if word_given in vocab_tokenization:\n",
    "    print('Tokenization of the known word:')\n",
    "    print(vocab_tokenization[word_given])\n",
    "    print('Tokenization treating the known word as unknown:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "else:\n",
    "    print('Tokenizating of the unknown word:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "___\n",
    "Me diverti mucho"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
