{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BYTE-PAIR-ENCODING del Quijote\n",
    "--- \n",
    "Este es una forma simple de compresión de datos en la que el par más común de bytes consecutivos de datos se reemplaza con un byte que no ocurre dentro de esos datos. Aquí, el objetivo no es la compresión de datos, sino la codificación de texto en un idioma dado como una secuencia de 'tokens', utilizando un vocabulario fijo de diferentes tokens. La mayoría de las palabras se codificarán como un solo token, mientras que las palabras raras se codificarán como una secuencia de unos pocos tokens, donde estos tokens representan partes de palabras significativas.\n",
    "\n",
    "### Equipo cangrejo      \n",
    "* Montaño Preciado Alondra Karolina\n",
    "* Velasquez Hidalgo Luis Juventino\n",
    "* Navarro Lopez Malcom Hiram\n",
    "* Faz Leal Juan Carlos\n",
    "\n",
    "\n",
    "### Fuentes\n",
    "- Medium con la informacion: https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0\n",
    "\n",
    "- Codigo en el cual nos estamos inspirando: https://leimao.github.io/blog/Byte-Pair-Encoding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_vocab()\n",
    "___\n",
    
    "La función get_vocab lee un archivo de texto (especificado por filename) y devuelve un diccionario de frecuencias de palabras (vocabulario).\n",
    "Cada línea en el archivo de texto se divide en palabras y cada palabra se agrega al vocabulario con un contador de frecuencia inicial de 1.\n", 
    "Si la palabra ya existe en el vocabulario, su contador se incrementa en 1.\n",
    
    "- filename: Es un argumento de entrada para la función, es el nombre (o la ruta) del archivo de texto que se va a leer para crear el vocabulario.\n",
    "- return: Es una instrucción que se utiliza para devolver un valor desde la funcion, en este caso, la función get_vocab devuelve el diccionario de frecuencias\n", 
    "\nde palabras (vocabulario) creado en el cuerpo de la función, que se almacena en la variable vocab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(filename):\n",
    "    '''\n",
    "    Parte el texto si encuentra espacios\n",
    "    '''\n",
    "    vocab = collections.defaultdict(int)\n",
    "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
    "        for line in fhand:\n",
    "            words = line.strip().split()\n",
    "            for word in words:\n",
    "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_stats()\n",
    "___\n",
    "Aqui se explica la funcion de esta celda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    '''\n",
    "    Devuelve el numero de veces que se repiten las palabras\n",
    "    '''\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge_vocab()\n",
    "___\n",
    "Funcion que junta los dos tokens que mas aparecen consecutivamente en el vocabulatio.\n",
    "El bigrama es la union de los dos tokens que mas aparecen. \n",
    "\n",
    "Si la palabra contiene el bigrama, \"w_out\" obtiene el valor de la palabra actual remplazando los dos tokens por el bigrama, de lo contrario \"w_out\" obtiene conserva el valor de la palabra de \n",
    "\n",
    "la instruccion:\n",
    "`v_out[w_out] = v_in[word]`\n",
    "nos asegura que la frecuencia de la *posible* nueva palabra se conserva.\n",
    "- pari: El par de tokens que mas se repiten  \n",
    "- v_in: El vocabulario actual de palabras.\n",
    "- return: El nuevo vocabulario, donde la ocurrencia del par de tokens 'pair' se remplazo por su union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_tokens_from_vocab()\n",
    "___\n",
    "Aqui se explica la funcion de esta celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_from_vocab(vocab):\n",
    "    tokens_frequencies = collections.defaultdict(int)\n",
    "    vocab_tokenization = {}\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens_frequencies[token] += freq\n",
    "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
    "    return tokens_frequencies, vocab_tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### measure_token_length()\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_token_length(token):\n",
    "    if token[-4:] == '</w>':\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        return len(token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui se explica la funcion de esta celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
    "    \n",
    "    if string == '':\n",
    "        return []\n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token]\n",
    "\n",
    "    string_tokens = []\n",
    "    for i in range(len(sorted_tokens)):\n",
    "        token = sorted_tokens[i]\n",
    "        token_reg = re.escape(token.replace('.', '[.]'))\n",
    "\n",
    "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
    "        if len(matched_positions) == 0:\n",
    "            continue\n",
    "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
    "\n",
    "        substring_start_position = 0\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "            string_tokens += [token]\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "        break\n",
    "    return string_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementacion usando el Quijote\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui implementamos el codigo de Byte-pair-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab('TextoEjemplo.txt')\n",
    "\n",
    "num_merges = 10000\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    print(\"Pares: \", pairs)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "    print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "    print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "print('==========')\n",
    "\n",
    "num_merges = 2\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "    print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "    print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "    print('==========')\n",
    "\n",
    "# Let's check how tokenization will be for a known word\n",
    "word_given_known = 'mountains</w>'\n",
    "word_given_unknown = 'Ilikeeatingapples!</w>'\n",
    "\n",
    "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
    "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
    "\n",
    "print(sorted_tokens)\n",
    "\n",
    "word_given = word_given_known \n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_given))\n",
    "if word_given in vocab_tokenization:\n",
    "    print('Tokenization of the known word:')\n",
    "    print(vocab_tokenization[word_given])\n",
    "    print('Tokenization treating the known word as unknown:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "else:\n",
    "    print('Tokenizating of the unknown word:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "\n",
    "word_given = word_given_unknown \n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_given))\n",
    "if word_given in vocab_tokenization:\n",
    "    print('Tokenization of the known word:')\n",
    "    print(vocab_tokenization[word_given])\n",
    "    print('Tokenization treating the known word as unknown:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "else:\n",
    "    print('Tokenizating of the unknown word:')\n",
    "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "___\n",
    "Me diverti mucho"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c43f423247bd0c67f6de4973e7e9f0055dbd5fe8e4d93d86b635664657df2b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
